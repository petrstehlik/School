\section{Implementation}
In this section, we describe the implementation phase of the project. The project was implemented using Python 2.7.13. External library SciPy\cite{scipy} was used for linear data interpolation.

\label{sec:implementation}

\subsection{Data Acquisition}
\label{sec:data-filter}
First, job data were queried and filtered according to several rules:
\begin{itemize}
    \item job run time must be between 10 and 60 minutes
    \item job must occupy the whole node (multiplies of 16 cores)
    \item job must be run in 15-day period
\end{itemize}

This resulted in 32 jobs which gave enough diversity for correct classification. All datapoints were aggregated by 30 seconds using averaging aggregator available in KairosDB.

The rule of minimum 10 minutes is because of any shorter job is usually a development, not production version of a program and in current HPC facilities such program is very cheap to run and therefore no deep performance analysis is needed. The same goes for jobs smaller than one node (16 cores).

Jobs longer than one hour are not suited for training the network because of too large input vectors exceeding the scope of this project and the loss of information in further data processing.

\subsection{Data Labelling}
In order to correctly label all chosen jobs and their metrics a simple graphical user interface was made. The GUI is based on previous work done during the PRACE Summer of HPC called Examon Web which is an extension of Examon framework visualizing specific views of all collected data.

Visualization is done in timeseries fashion using Dygraphs library. This helps to better understand the correlations between all metrics and in time combined.

The GUI is shown in figure \ref{fig:ex-labeler} where you can see a check box for each metric and at the bottom a checkbox for the whole job. The metric checkbox labels the accompanying metric of suspicious behaviour and job one of suspicious behaviour of the job as a whole.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{examon-labeler}
    \caption{Part of Examon Web based data labelling tool}
    \label{fig:ex-labeler}
\end{figure}

\subsection{Data Processing}

After labelling the job all metric data with labels are generated and can be worked on further. All metric vectors were interpolated to 80 values which provides good performance/detail compromise. Larger input vectors resulted in extremely longer runs and smaller input vectors resulted in poor outputs of the network.

\subsection{Metric Networks}

To achieve best results/speed combination a backpropagation neural network was created for each metric. This gives us the total of twelve networks completely independent of each other which means the training process can be fully parallelized.

All metric networks were configured the same way to achieve uniform results. The input layer disposes of 80 neurons with hidden layers of 4 and then 3 neurons and one output neuron.

This configuration was chosen based on trial and error experiments.

\subsection{Job Network}
Once all metric networks compute their output we have a vector of 12 values which serves as an input to the job classification network.

The network is created with 12 input neurons, one hidden layer of 4 neurons and one output neuron.

All networks output a single value. Metric networks because of further processing in the job network and the job network because of clearer values.
